{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.19.5 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python37\\site-packages (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.19.5 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py==2.10.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python37\\site-packages (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python37\\site-packages (from h5py==2.10.0) (1.19.5)\n",
      "Requirement already satisfied: six in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from h5py==2.10.0) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py==2.10.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==3.6.0\n",
      "  Using cached gensim-3.6.0.tar.gz (23.1 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python37\\site-packages (from gensim==3.6.0) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from gensim==3.6.0) (1.6.2)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from gensim==3.6.0) (1.15.0)\n",
      "Collecting smart_open>=1.2.1\n",
      "  Using cached smart_open-5.1.0-py3-none-any.whl (57 kB)\n",
      "Building wheels for collected packages: gensim\n",
      "  Building wheel for gensim (setup.py): started\n",
      "  Building wheel for gensim (setup.py): finished with status 'done'\n",
      "  Created wheel for gensim: filename=gensim-3.6.0-cp37-cp37m-win_amd64.whl size=23429695 sha256=3321bcf0d08c9ec1e41bc09f25bdd4ce324610a9cf0005f026657ac556624a44\n",
      "  Stored in directory: c:\\users\\natthawattungruethai\\appdata\\local\\pip\\cache\\wheels\\53\\c8\\f9\\afb722099bdb5d73e5807019ce1512fd065502ccc15ea2b5bd\n",
      "Successfully built gensim\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.6.0 smart-open-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==3.6.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4035,
     "status": "ok",
     "timestamp": 1620716501332,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "wxRqFag-N8dW",
    "outputId": "b5211c30-c91c-472d-f826-057ad5c61271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepcut\n",
      "  Using cached deepcut-0.7.0.0-py3-none-any.whl (2.0 MB)\n",
      "Requirement already satisfied: tensorflow>=2.0.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from deepcut) (2.1.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from deepcut) (0.24.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from deepcut) (1.2.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from deepcut) (1.6.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python37\\site-packages (from deepcut) (2.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python37\\site-packages (from deepcut) (1.19.5)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (3.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (2.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (0.10.0)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.4.1-cp37-cp37m-win_amd64.whl (30.9 MB)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (0.8.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (3.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (0.2.2)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Using cached tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.31.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.0.8)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (0.36.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.12.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from protobuf>=3.8.0->tensorflow>=2.0.0->deepcut) (52.0.0.post20210125)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (3.3.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (1.22.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (0.16.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (2.25.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (1.25.11)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->deepcut) (3.3.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from pandas->deepcut) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from pandas->deepcut) (2.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from scikit-learn->deepcut) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from scikit-learn->deepcut) (1.0.1)\n",
      "Installing collected packages: tensorboard, scipy, deepcut\n",
      "Successfully installed deepcut-0.7.0.0 scipy-1.4.1 tensorboard-2.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\NATTHAWATTUNGRUETHAI\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install deepcut --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5877,
     "status": "ok",
     "timestamp": 1620716503182,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "rHO_3T9BOGui",
    "outputId": "1a44454f-1da2-4648-cdbb-c18110a79609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pythainlp\n",
      "  Using cached pythainlp-2.3.1-py3-none-any.whl (11.0 MB)\n",
      "Collecting tinydb>=3.0\n",
      "  Using cached tinydb-4.4.0-py3-none-any.whl (21 kB)\n",
      "Collecting python-crfsuite>=0.9.6\n",
      "  Downloading python_crfsuite-0.9.7-cp37-cp37m-win_amd64.whl (154 kB)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from pythainlp) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests>=2.22.0->pythainlp) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
      "Installing collected packages: tinydb, python-crfsuite, pythainlp\n",
      "Successfully installed pythainlp-2.3.1 python-crfsuite-0.9.7 tinydb-4.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script thainlp.exe is installed in 'C:\\Users\\NATTHAWATTUNGRUETHAI\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install pythainlp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.4.4-cp37-cp37m-win_amd64.whl (269 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.6.2 regex-2021.4.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script nltk.exe is installed in 'C:\\Users\\NATTHAWATTUNGRUETHAI\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G9D118ZAOZZB"
   },
   "outputs": [],
   "source": [
    "from pythainlp.corpus import thai_stopwords\n",
    "import deepcut\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask_ngrok import run_with_ngrok\n",
    "from flask import Flask, jsonify, request\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5qw6hsPwOmcD"
   },
   "outputs": [],
   "source": [
    "#Clean Text\n",
    "def remove_repettition(text):\n",
    "    token_list = list(text)\n",
    "    if len(token_list) > 2:\n",
    "        filter_list = [True, True]\n",
    "        n = len(token_list)\n",
    "        for i in range(2, n):\n",
    "            if (token_list[i] == token_list[i-1]) and (token_list[i] == token_list[i-2]):\n",
    "                filter_list.append(False)\n",
    "            else:\n",
    "                filter_list.append(True)\n",
    "\n",
    "        output = ''.join(np.array(token_list)[filter_list])\n",
    "    else:\n",
    "        output = text\n",
    "    return output\n",
    "\n",
    "def cleansing(text):\n",
    "    # \\t, \\n, \\xa0 and other special characters. Replace by blank string\n",
    "    text = re.sub('[\\t\\n\\xa0\\\"\\'!?\\/\\(\\)%\\:\\=\\-\\+\\*\\_ๆ]', '', text)\n",
    "    \n",
    "    # Numbers. Replace by space\n",
    "    text = re.sub('[0-9]', ' ', text)\n",
    "    \n",
    "    # Dot. Replace by space\n",
    "    text = re.sub('[\\.]', ' ', text)\n",
    "    \n",
    "    # One or more consecutive space. Replace by single space\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    # Remove 2 or more repettition\n",
    "    text = remove_repettition(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NNqVro6LPo8a"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "wv_model = gensim.models.Word2Vec.load('corpus.th.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6cfjBM4tP4UK"
   },
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    index = 0\n",
    "    index = wv_model.wv.vocab[word].index\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oIAJw0p7P7XZ"
   },
   "outputs": [],
   "source": [
    "def word_index(listword):\n",
    "    dataset = []\n",
    "    vocabulary = dict()\n",
    "    inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "    for sentence in listword:\n",
    "        tmp = []\n",
    "        for w in sentence:\n",
    "            if w not in wv_model:\n",
    "                continue\n",
    "\n",
    "            if w not in vocabulary:\n",
    "                vocabulary[w] = len(inverse_vocabulary)\n",
    "                tmp.append(len(inverse_vocabulary))\n",
    "                inverse_vocabulary.append(w)\n",
    "            else:\n",
    "                tmp.append(word2idx(w))\n",
    "        dataset.append(tmp)\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LO-WyseOP-li"
   },
   "outputs": [],
   "source": [
    "# define word embedding\n",
    "vocab_list = [(k, wv_model.wv[k]) for k, v in wv_model.wv.vocab.items()]\n",
    "embeddings_matrix = np.zeros((len(wv_model.wv.vocab.items()) + 1, wv_model.vector_size))\n",
    "for i in range(len(vocab_list)):\n",
    "    word = vocab_list[i][0]\n",
    "    embeddings_matrix[i + 1] = vocab_list[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1LVdY7-KDCP9EF2jZebOawVR_3jUQnSGM"
    },
    "executionInfo": {
     "elapsed": 6358,
     "status": "ok",
     "timestamp": 1620716838058,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "YzIv6bWzQC-a",
    "outputId": "14f29683-2dc8-45ec-f731-2f7f617dc726"
   },
   "outputs": [],
   "source": [
    "# vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XdooUj6QQITq"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embeddings_matrix = 1 * np.random.randn(len(vocab_list) + 1, EMBEDDING_DIM)  # This will be the embedding matrix\n",
    "embeddings_matrix[0] = 0  # So that the padding will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0FdjJe_VPdZ0"
   },
   "outputs": [],
   "source": [
    "# Model variables\n",
    "n_hidden = 256\n",
    "batch_size = 128\n",
    "n_epoch = 100\n",
    "max_seq_length = 2704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1620716858559,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "eqK0ymH9QKRy",
    "outputId": "31ac3c38-d119-42ac-f01c-2d5c3a8a06fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.8175345 , -0.04716729,  0.17979133, ...,  0.89611967,\n",
       "         0.50152011, -2.86750639],\n",
       "       [-0.15279231,  0.62939607,  0.81142454, ..., -0.08821857,\n",
       "         1.49437165,  1.1397306 ],\n",
       "       ...,\n",
       "       [-0.69642473,  0.0290668 , -1.14854544, ..., -1.32213359,\n",
       "         0.30663623, -0.45694847],\n",
       "       [ 1.01581125, -0.85774445,  0.53233115, ...,  0.60640623,\n",
       "        -0.20361701, -1.07477053],\n",
       "       [ 1.75399242, -0.9872687 ,  0.93163857, ..., -0.96259091,\n",
       "        -1.63109266,  1.7665071 ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "an1uk86LQTio"
   },
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JgPsQ8MlPfWA"
   },
   "outputs": [],
   "source": [
    "# The visible layer\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(len(embeddings_matrix), EMBEDDING_DIM, weights=[embeddings_matrix], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "# Embedded version of the inputs\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_lstm = LSTM(n_hidden)\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "# Pack it all up into a model\n",
    "malstm = Model([left_input, right_input], [malstm_distance])\n",
    "\n",
    "\n",
    "malstm.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Start training\n",
    "training_start_time = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1620717012710,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "Nh9milBGQpYY",
    "outputId": "6383054e-67f5-410f-8332-50281e0c54f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2704)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2704)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 2704, 300)    9468300     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          570368      embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "==================================================================================================\n",
      "Total params: 10,038,668\n",
      "Trainable params: 570,368\n",
      "Non-trainable params: 9,468,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "malstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JUOlaUM6OylG"
   },
   "outputs": [],
   "source": [
    "# Load best weight from model\n",
    "malstm.load_weights('sm_colab_ka.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y69P6GtuXAaB"
   },
   "source": [
    "#Test with Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Kjaib296O0US"
   },
   "outputs": [],
   "source": [
    "def prepare_for_predict(input_questions):\n",
    "    q_input= []\n",
    "    cleansing(input_questions)\n",
    "    tokenized_input_1 =deepcut.tokenize(input_questions)\n",
    "    for sentence in tokenized_input_1:\n",
    "      q_input.append(sentence)\n",
    "    q_input= word_index(tokenized_input_1)\n",
    "    q_input = pad_sequences(q_input, maxlen=max_seq_length)\n",
    "    return q_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "w_EAiigkZ51b"
   },
   "outputs": [],
   "source": [
    "#Duplicate list\n",
    "def duplicate(testList, n):\n",
    "    return [ele for ele in testList for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pL8GEUFCWEKq"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def tokenize_text_list(ls):\n",
    "    \"\"\"Tokenize list of text\"\"\"\n",
    "    return list(chain.from_iterable([deepcut.tokenize(ls)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#หลักสูตร\n",
    "curriculum = ['วิชา CPE332/Professional issuesมีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิชาเลือกที่มีวิชาบังคับลงก่อน หลักสูตรปกติ?',\n",
    "  'วิศวะคอม/หลักสูตรนานาชาติ ปี 4 เทอม 2\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวะคอม/หลักสูตรวิทยาศาสตรบัณฑิต สาขาวิชาวิทยาศาสตร์ข้อมูลสุขภาพ \\nปี 4 เทอม 2 ต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิชาเลือกที่มีวิชาบังคับลงก่อน หลักสูตรวิทยาศาสตรบัณฑิต สาขาวิชาวิทยาศาสตร์ข้อมูลสุขภาพ ?',\n",
    "  'วิชา CPE375/Interactive computingมีวิชาตัวต่อมั้ยครับ?',\n",
    "  'หน่วยกิตที่ต้องเก็บให้ครบก่อนจบหรือไม่?',\n",
    "  'วิศวะคอม/วิศวกรรมคอมพิวเตอหลักสูตรวิทยาศาสตรบัณฑิต สาขาวิชาวิทยาศาสตร์ข้อมูลสุขภาพ \\nปี 3 เทอม 2 ต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิชา CPE224/Computer architectures มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิศวะคอม/หลักสูตรนานาชาติปี 1 เทอม 2\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวะคอม/หลักสูตรนานาชาติปี 1 เทอม 1 \\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิชาเลือกที่มีวิชาบังคับลงก่อน หลักสูตรนานาชาติ?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรวิทยาศาตร์ข้อมูลสุขภาพ มีกี่หน่วยกิต? ',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรปกติ ปีที่1-2 เรียนอะไรบ้าง?',\n",
    "  'วิศวะคอม/หลักสูตรนานาชาติปี 2 เทอม 2\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิชา CPE212/Algorithm มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิศวะคอม/หลักสูตรปกติ  ปี 4 เทอม 1\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวะคอมหลักสูตรวิทยาศาตร์ข้อมูลสุขภาพมีอัตราค่าเรียนเท่าไหร่',\n",
    "  'วิศวะคอม/หลักสูตรวิทยาศาสตรบัณฑิต สาขาวิชาวิทยาศาสตร์ข้อมูลสุขภาพ \\nปี 3 เทอม 1 ต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิชาภาคบังคับที่มีวิชาต่อเนื่อง มีอะไรบ้าง หลักสูตรปกติ?',\n",
    "  'วิชา CPE314/Computer networksมีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรเรสิเดนทอล คอเลจ ปีที่ 1-2 เรียนอะไรบ้าง?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรวิทยาศาตร์ข้อมูลสุขภาพ ปีที่ 3 เรียนอะไรบ้าง?',\n",
    "  'วิชา CPE327/Software engineering มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิศวะคอมพิวเตอร์แต่ละหลักสูตรมีระยะเวลาในการศึกษากี่ปี?',\n",
    "  'วิชา CPE325/Big data มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิชา CPE111/Data Structure มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิศวะคอม/หลักสูตรปกติ ปี 2 เทอม 1\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวะคอม/หลักสูตรนานาชาติ ปี 2 เทอม 1\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิชาเลือกเสรีต้องลงวิชานอกภาคเท่านั้นหรือไม่?',\n",
    "  'วิศวะคอมหลักสูตรปกติมีอัตราค่าเรียนเท่าไหร่',\n",
    "  'วิชา CPE326/Operating systems มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิชา CPE121/Discrete มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรวิทยาศาตร์ข้อมูลสุขภาพ ปีที่ 4 เรียนอะไรบ้าง?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรปกติ มีกี่หน่วยกิต?',\n",
    "  'วิชา CPE342/Java programmingมีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิชาภาษาอังกฤษ LNG มีเกณท์การเรื่มเรียนอย่างไร หลักสูตรวิทยาศาสตรบัณฑิต\\nสาขาวิชาวิทยาศาสตร์ข้อมูลสุขภาพ ?',\n",
    "  'วิชา CPE343/Object orientedมีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิชา CPE100/Programming มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรนานาชาติ มีกี่หน่วยกิต?',\n",
    "  'วิศวะคอม/หลักสูตรวิทยาศาสตรบัณฑิต สาขาวิชาวิทยาศาสตร์ข้อมูลสุขภาพ \\nปี 1 เทอม 1 ต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวะคอม/หลักสูตรปกติ ปี 3 เทอม 1\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรนานาชาติ ปีที่1-2 เรียนอะไรบ้าง?',\n",
    "  'วิชา CPE223/Digital มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิศวะคอม/หลักสูตรนานาชาติปี 4 เทอม 1\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิชา CPE101/Exploration มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรเรสิเดนทอล คอเลจ ปีที่ 3-4 เรียนอะไรบ้าง?',\n",
    "  'วิศวะคอม/หลักสูตรนานาชาติ ปี 3 เทอม 2\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิชาภาษาอังกฤษ LNG มีเกณท์การเรื่มเรียนอย่างไร หลักสูตรปกติ ?',\n",
    "  'วิชาภาคบังคับที่มีตัวต่อเนื่องมีอะไรบ้าง หลักสูตรนานาชาติ?',\n",
    "  'วิศวะคอม/หลักสูตรปกติ  ปี 3 เทอม 2\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรวิทยาศาตร์ข้อมูลสุขภาพ ปีที่1-2 เรียนอะไรบ้าง?',\n",
    "  'วิชา CPE231/Database มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิชาภาคบังคับที่มีวิชาต่อเนื่อง  หลักสูตรวิทยาศาสตรบัณฑิต สาขาวิชาวิทยาศาสตร์ข้อมูลสุขภาพ \\nมีอะไรบ้าง ?',\n",
    "  'วิชาภาษาอังกฤษ LNG มีเกณท์การเรื่มเรียนอย่างไร หลักสูตรนานาชาติ ?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรปกติ ปีที่ 3 เรียนอะไรบ้าง?',\n",
    "  'วิศวะคอม/หลักสูตรปกติ  ปี 2 เทอม 2\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวะคอม/หลักสูตรปกติ ปี 1 เทอม 2\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวะคอมหลักสูตรปกติมีระยะเวลาในการศึกษากี่ปี?',\n",
    "  'วิศวะคอม/หลักสูตรวิทยาศาสตรบัณฑิต สาขาวิชาวิทยาศาสตร์ข้อมูลสุขภาพ \\nปี 2 เทอม 1 ต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิชา CPE213/Data model วิชาตัวต่อมั้ยครับ?',\n",
    "  'วิศวะคอม/หลักสูตรวิทยาศาสตรบัณฑิต สาขาวิชาวิทยาศาสตร์ข้อมูลสุขภาพ \\nปี 1 เทอม 2 ต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวะคอม/ มีทั้งหมดกี่หลักสูตร? มีหลักสูตรอะไรบ้าง?',\n",
    "  'วิศวะคอมหลักสูตรนานาชาติมีอัตราค่าเรียนเท่าไหร่',\n",
    "  'วิศวะคอม/หลักสูตรวิทยาศาสตรบัณฑิต สาขาวิชาวิทยาศาสตร์ข้อมูลสุขภาพ \\nปี 4 เทอม 1 ต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรนานาชาติ ปีที่ 4 เรียนอะไรบ้าง?',\n",
    "  'วิชา CPE122/Circuits มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิศวะคอม/หลักสูตรปกติ  ปี 4 เทอม 2\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรนานาชาติ ปีที่ 3 เรียนอะไรบ้าง?',\n",
    "  'วิศวะคอม/หลักสูตรนานาชาติ ปี 3 เทอม 1\\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิชา CPE329/Business intelligence มีวิชาตัวต่อมั้ยครับ?',\n",
    "  'วิศวะคอม/หลักสูตรวิทยาศาสตรบัณฑิต สาขาวิชาวิทยาศาสตร์ข้อมูลสุขภาพ \\nปี 2 เทอม 2 ต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?',\n",
    "  'วิศวกรรมคอมพิวเตอร์หลักสูตรปกติ ปีที่ 4 เรียนอะไรบ้าง?',\n",
    "  'วิศวะคอม/หลักสูตรปกติ ปี 1 เทอม 1 \\nต้องลงเรียนกี่หน่วยกิต? เรียนวิชาอะไรบ้าง?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# time total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NATTHAWATTUNGRUETHAI\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\NATTHAWATTUNGRUETHAI\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: วิศวะคอม/ มีทั้งหมดกี่หลักสูตร? มีหลักสูตรอะไรบ้าง? 0.996301\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "curriculumObj = {'curriculum':curriculum}\n",
    "curriculumDF = pd.DataFrame(data=curriculumObj)\n",
    "beforeTok = {}\n",
    "beforeTok['หลักสูตร'] = curriculumDF\n",
    "tokenized_curriculum =curriculumDF.curriculum.map(tokenize_text_list)\n",
    "max_word = 19219\n",
    "max_seq_length = 2704\n",
    "q_category= []\n",
    "for sentence in tokenized_curriculum:\n",
    "    q_category.append(sentence)\n",
    "q_category = word_index(q_category)\n",
    "all_Question_categorylen = len(q_category)\n",
    "all_Question_categorylen\n",
    "input2 = 'วิศวคอมมีหลักสูตรอะไรบ้าง'\n",
    "tokenized_input_2=cleansing(input2)\n",
    "tokenized_input_2 =deepcut.tokenize(tokenized_input_2)\n",
    "tokenized_dup_input_2= duplicate([tokenized_input_2],all_Question_categorylen)\n",
    "q_user = word_index(tokenized_dup_input_2)\n",
    "# Split to dicts\n",
    "M_input = {'left': q_category, 'right': q_user}\n",
    "# Zero padding\n",
    "for model_input, side in itertools.product([M_input], ['left', 'right']):\n",
    "    model_input[side] = pad_sequences(model_input[side], maxlen=max_seq_length)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert M_input['left'].shape == M_input['right'].shape\n",
    "play_predict = malstm.predict(x=[M_input['left'],  M_input['right']])\n",
    "max_question_percentage = max(play_predict)\n",
    "question_index = np.where(play_predict == max_question_percentage)\n",
    "predictedQuestion = curriculum[question_index[0][0]]\n",
    "print('output: '+ predictedQuestion+ ' ' + \"%lf\" % max_question_percentage)\n",
    "value = {\n",
    "  \"predictedQuestion\": predictedQuestion,\n",
    "  \"similarity\": \"%lf\" % max_question_percentage\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# time seperate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "curriculumObj = {'curriculum':curriculum}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "curriculumDF = pd.DataFrame(data=curriculumObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "beforeTok = {}\n",
    "beforeTok['หลักสูตร'] = curriculumDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BsY51Xn9WTBS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_curriculum =curriculumDF.curriculum.map(tokenize_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "CGdqwM2qWt0r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_word = 19219\n",
    "max_seq_length = 2704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "1XpFqeXNWuux"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q_category= []\n",
    "for sentence in tokenized_curriculum:\n",
    "    q_category.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1780,
     "status": "ok",
     "timestamp": 1620718746721,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "UAWaItX5VMgx",
    "outputId": "b98d6b35-47da-474b-c57f-761560da32ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.97 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NATTHAWATTUNGRUETHAI\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\NATTHAWATTUNGRUETHAI\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q_category = word_index(q_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 983,
     "status": "ok",
     "timestamp": 1620719336702,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "tuC_yktbZW3r",
    "outputId": "1fc8fba7-ff9c-422d-cf7b-c9bb68e8efb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "all_Question_categorylen = len(q_category)\n",
    "all_Question_categorylen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GHSbyPrYTtR"
   },
   "source": [
    "#Question from User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "9hiwFzS5YSwC"
   },
   "outputs": [],
   "source": [
    "input2 = 'วิศวคอมมีหลักสูตรอะไรบ้าง'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "6brFZaZWYjiM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_input_2=cleansing(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_input_2 =deepcut.tokenize(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 945,
     "status": "ok",
     "timestamp": 1620719906256,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "mWT7jP6eaWrq",
    "outputId": "2228f3a8-cf2b-41a5-cc34-7e074a6b24de",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_dup_input_2= duplicate([tokenized_input_2],all_Question_categorylen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1620720299412,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "KWz3M_DaYQAh",
    "outputId": "a1c2da27-e31e-4507-9eb3-87ca3160217c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.99 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NATTHAWATTUNGRUETHAI\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q_user = word_index(tokenized_dup_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "IJsfJNSmdO4P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Split to dicts\n",
    "M_input = {'left': q_category, 'right': q_user}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "0zcdoqaUd_Zm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 997 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Zero padding\n",
    "for model_input, side in itertools.product([M_input], ['left', 'right']):\n",
    "    model_input[side] = pad_sequences(model_input[side], maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make sure everything is ok\n",
    "assert M_input['left'].shape == M_input['right'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "QQjSaocQeck_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 673 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "play_predict = malstm.predict(x=[M_input['left'],  M_input['right']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_question_percentage = max(play_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question_index = np.where(play_predict == max_question_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 997 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictedQuestion = beforeTok['หลักสูตร'].loc[question_index[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 998 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "value = {\n",
    "          \"predictedQuestion\": str(predictedQuestion),\n",
    "          \"similarity\": \"%lf\" % max_question_percentage\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'{\"predictedQuestion\": \"curriculum    \\xe0\\xb8\\xa7\\xe0\\xb8\\xb4\\xe0\\xb8\\xa8\\xe0\\xb8\\xa7\\xe0\\xb8\\xb0\\xe0\\xb8\\x84\\xe0\\xb8\\xad\\xe0\\xb8\\xa1/ \\xe0\\xb8\\xa1\\xe0\\xb8\\xb5\\xe0\\xb8\\x97\\xe0\\xb8\\xb1\\xe0\\xb9\\x89\\xe0\\xb8\\x87\\xe0\\xb8\\xab\\xe0\\xb8\\xa1\\xe0\\xb8\\x94\\xe0\\xb8\\x81\\xe0\\xb8\\xb5\\xe0\\xb9\\x88\\xe0\\xb8\\xab\\xe0\\xb8\\xa5\\xe0\\xb8\\xb1\\xe0\\xb8\\x81\\xe0\\xb8\\xaa\\xe0\\xb8\\xb9\\xe0\\xb8\\x95\\xe0\\xb8\\xa3? \\xe0\\xb8\\xa1\\xe0\\xb8\\xb5\\xe0\\xb8\\xab\\xe0\\xb8\\xa5\\xe0\\xb8\\xb1\\xe0\\xb8\\x81\\xe0\\xb8\\xaa\\xe0\\xb8\\xb9\\xe0\\xb8\\x95\\xe0\\xb8\\xa3\\xe0\\xb8\\xad\\xe0\\xb8\\xb0\\xe0\\xb9\\x84\\xe0\\xb8\\xa3...\\\\nName: 62, dtype: object\", \"similarity\": \"0.996301\"}'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "json.dumps(value, ensure_ascii=False).encode('utf8')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Siamese_malstm_play.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
