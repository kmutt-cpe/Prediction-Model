{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                Version\n",
      "---------------------- -------------------\n",
      "absl-py                0.10.0\n",
      "aiohttp                3.6.3\n",
      "argon2-cffi            20.1.0\n",
      "astor                  0.8.1\n",
      "async-generator        1.10\n",
      "async-timeout          3.0.1\n",
      "attrs                  20.2.0\n",
      "backcall               0.2.0\n",
      "bayesian-optimization  1.2.0\n",
      "bleach                 3.3.0\n",
      "blinker                1.4\n",
      "boto3                  1.17.46\n",
      "botocore               1.20.78\n",
      "brotlipy               0.7.0\n",
      "cachetools             4.1.1\n",
      "certifi                2020.12.5\n",
      "cffi                   1.14.3\n",
      "chardet                3.0.4\n",
      "click                  7.1.2\n",
      "cloudpickle            1.6.0\n",
      "colorama               0.4.4\n",
      "cryptography           3.1.1\n",
      "cycler                 0.10.0\n",
      "decorator              5.0.9\n",
      "deepcut                0.7.0.0\n",
      "defusedxml             0.7.1\n",
      "entrypoints            0.3\n",
      "et-xmlfile             1.1.0\n",
      "Flask                  1.1.2\n",
      "gast                   0.2.2\n",
      "gensim                 3.6.0\n",
      "google-auth            1.22.1\n",
      "google-auth-oauthlib   0.4.1\n",
      "google-pasta           0.2.0\n",
      "grpcio                 1.31.0\n",
      "gym                    0.18.3\n",
      "h5py                   2.10.0\n",
      "idna                   2.10\n",
      "importlib-metadata     2.0.0\n",
      "ipykernel              5.3.4\n",
      "ipython                7.22.0\n",
      "ipython-genutils       0.2.0\n",
      "ipywidgets             7.6.3\n",
      "itsdangerous           2.0.1\n",
      "jedi                   0.17.0\n",
      "Jinja2                 3.0.0\n",
      "jmespath               0.10.0\n",
      "joblib                 1.0.1\n",
      "jsonschema             3.0.2\n",
      "jupyter                1.0.0\n",
      "jupyter-client         6.1.12\n",
      "jupyter-console        6.4.0\n",
      "jupyter-core           4.7.1\n",
      "jupyterlab-pygments    0.1.2\n",
      "jupyterlab-widgets     1.0.0\n",
      "kaggle                 1.5.12\n",
      "Keras-Applications     1.0.8\n",
      "Keras-Preprocessing    1.1.0\n",
      "kiwisolver             1.3.1\n",
      "lxml                   4.6.3\n",
      "Markdown               3.3.2\n",
      "MarkupSafe             2.0.1\n",
      "matplotlib             3.3.4\n",
      "mistune                0.8.4\n",
      "mkl-fft                1.2.0\n",
      "mkl-random             1.1.0\n",
      "mkl-service            2.3.0\n",
      "multidict              4.7.6\n",
      "nbclient               0.5.3\n",
      "nbconvert              6.0.7\n",
      "nbformat               5.1.3\n",
      "nest-asyncio           1.5.1\n",
      "nltk                   3.6.2\n",
      "notebook               6.4.0\n",
      "numpy                  1.19.5\n",
      "oauthlib               3.1.0\n",
      "olefile                0.46\n",
      "openpyxl               3.0.7\n",
      "opt-einsum             3.1.0\n",
      "packaging              20.9\n",
      "pandas                 1.2.4\n",
      "pandas-datareader      0.9.0\n",
      "pandocfilters          1.4.3\n",
      "parso                  0.8.2\n",
      "pickleshare            0.7.5\n",
      "Pillow                 8.2.0\n",
      "pip                    21.1.1\n",
      "prometheus-client      0.10.1\n",
      "prompt-toolkit         3.0.17\n",
      "protobuf               3.13.0\n",
      "pyasn1                 0.4.8\n",
      "pyasn1-modules         0.2.8\n",
      "pycparser              2.20\n",
      "pyglet                 1.5.15\n",
      "Pygments               2.9.0\n",
      "PyJWT                  1.7.1\n",
      "pyOpenSSL              19.1.0\n",
      "pyparsing              2.4.7\n",
      "pyrsistent             0.17.3\n",
      "PySocks                1.7.1\n",
      "pythainlp              2.3.1\n",
      "python-crfsuite        0.9.7\n",
      "python-dateutil        2.8.1\n",
      "python-slugify         5.0.2\n",
      "pytz                   2021.1\n",
      "pywin32                227\n",
      "pywinpty               0.5.7\n",
      "PyYAML                 5.4.1\n",
      "pyzmq                  20.0.0\n",
      "qtconsole              5.0.3\n",
      "QtPy                   1.9.0\n",
      "regex                  2021.4.4\n",
      "requests               2.25.1\n",
      "requests-oauthlib      1.3.0\n",
      "rsa                    4.6\n",
      "s3transfer             0.3.6\n",
      "scikit-learn           0.24.2\n",
      "scipy                  1.4.1\n",
      "Send2Trash             1.5.0\n",
      "setuptools             52.0.0.post20210125\n",
      "six                    1.15.0\n",
      "smart-open             5.1.0\n",
      "tensorboard            2.1.1\n",
      "tensorboard-plugin-wit 1.6.0\n",
      "tensorflow             2.1.0\n",
      "tensorflow-estimator   2.1.0\n",
      "termcolor              1.1.0\n",
      "terminado              0.9.4\n",
      "testpath               0.4.4\n",
      "text-unidecode         1.3\n",
      "threadpoolctl          2.1.0\n",
      "tinydb                 4.4.0\n",
      "tornado                6.1\n",
      "tqdm                   4.59.0\n",
      "traitlets              5.0.5\n",
      "urllib3                1.25.11\n",
      "wcwidth                0.2.5\n",
      "webencodings           0.5.1\n",
      "Werkzeug               0.16.1\n",
      "wheel                  0.36.2\n",
      "widgetsnbextension     3.5.1\n",
      "win-inet-pton          1.1.0\n",
      "wincertstore           0.2\n",
      "wrapt                  1.12.1\n",
      "xlrd                   2.0.1\n",
      "yarl                   1.6.2\n",
      "zipp                   3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.19.5 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.19.5 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py==2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from h5py==2.10.0) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from h5py==2.10.0) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py==2.10.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.6.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (3.6.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from gensim==3.6.0) (5.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim==3.6.0) (1.5.2)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim==3.6.0) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from gensim==3.6.0) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==3.6.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4035,
     "status": "ok",
     "timestamp": 1620716501332,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "wxRqFag-N8dW",
    "outputId": "b5211c30-c91c-472d-f826-057ad5c61271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepcut in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (0.7.0.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from deepcut) (1.1.3)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from deepcut) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from deepcut) (0.23.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from deepcut) (1.19.5)\n",
      "Requirement already satisfied: tensorflow>=2.0.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from deepcut) (2.4.1)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from deepcut) (2.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->deepcut) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->deepcut) (2020.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->deepcut) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->deepcut) (2.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (3.3.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (3.16.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (0.12.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (1.1.2)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.32.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.12)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (0.3.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (2.5.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (2.4.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (0.2.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.15.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (0.35.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (3.7.4.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (1.30.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (50.3.1.post20201107)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (3.3.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (4.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install deepcut --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5877,
     "status": "ok",
     "timestamp": 1620716503182,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "rHO_3T9BOGui",
    "outputId": "1a44454f-1da2-4648-cdbb-c18110a79609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pythainlp in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (2.3.1)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from pythainlp) (0.9.7)\n",
      "Requirement already satisfied: tinydb>=3.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from pythainlp) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pythainlp) (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pythainlp) (1.25.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install pythainlp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "G9D118ZAOZZB"
   },
   "outputs": [],
   "source": [
    "#prediction\n",
    "import deepcut\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Lambda\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question import\n",
    "import requests\n",
    "\n",
    "# Category model import\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask_ngrok import run_with_ngrok\n",
    "from flask import Flask\n",
    "import threading \n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>หลักสูตร</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>คำถามทั่วไป</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ลงทะเบียนเรียน</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>การรับเข้านักศึกษา</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Category\n",
       "0            หลักสูตร\n",
       "1         คำถามทั่วไป\n",
       "2      ลงทะเบียนเรียน\n",
       "3  การรับเข้านักศึกษา"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"Category.xlsx\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load File\n",
    "with open('token_text_category.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    tokenized_texts = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pL8GEUFCWEKq"
   },
   "outputs": [],
   "source": [
    "def tokenize_text_list(ls):\n",
    "    \"\"\"Tokenize list of text\"\"\"\n",
    "    return list(chain.from_iterable([deepcut.tokenize(ls)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bow(tokenized_text, vocabulary_):\n",
    "    n_doc = len(tokenized_text)\n",
    "    values, row_indices, col_indices = [], [], []\n",
    "    for r, tokens in enumerate(tokenized_text):\n",
    "        feature = {}\n",
    "        for token in tokens:\n",
    "            word_index = vocabulary_.get(token)\n",
    "            if word_index is not None:\n",
    "                if word_index not in feature.keys():\n",
    "                    feature[word_index] = 1\n",
    "                else:\n",
    "                    feature[word_index] += 1\n",
    "        for c, v in feature.items():\n",
    "            values.append(v)\n",
    "            row_indices.append(r)\n",
    "            col_indices.append(c)\n",
    "        #print(feature)\n",
    "\n",
    "    # document-term matrix in sparse CSR format\n",
    "    X = sp.csr_matrix((values, (row_indices, col_indices)),\n",
    "                      shape=(n_doc, len(vocabulary_)))\n",
    "    return X\n",
    "\n",
    "vocabulary_ = {v: k for k, v in enumerate(set(chain.from_iterable(tokenized_texts)))}\n",
    "X = text_to_bow(tokenized_texts, vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "svd_model = TruncatedSVD(n_components=100,\n",
    "                         algorithm='arpack', n_iter=100)\n",
    "X_tfidf = transformer.fit_transform(X)\n",
    "X_svd = svd_model.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = pd.get_dummies(data.Category).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NATTHAWATTUNGRUETHAI\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#Load Model\n",
    "logist_models = joblib.load(\"category_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['การรับเข้านักศึกษา', 'คำถามทั่วไป', 'ลงทะเบียนเรียน', 'หลักสูตร'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(np.vstack([model.predict_proba(X_svd)[:, 1] for model in logist_models]).T, axis=1)\n",
    "y_pred = np.array([tag[yi] for yi in y_pred])\n",
    "y_true = data.Category.values\n",
    "print(tag[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5qw6hsPwOmcD"
   },
   "outputs": [],
   "source": [
    "#Clean Text\n",
    "def remove_repettition(text):\n",
    "    token_list = list(text)\n",
    "    if len(token_list) > 2:\n",
    "        filter_list = [True, True]\n",
    "        n = len(token_list)\n",
    "        for i in range(2, n):\n",
    "            if (token_list[i] == token_list[i-1]) and (token_list[i] == token_list[i-2]):\n",
    "                filter_list.append(False)\n",
    "            else:\n",
    "                filter_list.append(True)\n",
    "\n",
    "        output = ''.join(np.array(token_list)[filter_list])\n",
    "    else:\n",
    "        output = text\n",
    "    return output\n",
    "\n",
    "def cleansing(text):\n",
    "    # \\t, \\n, \\xa0 and other special characters. Replace by blank string\n",
    "    text = re.sub('[\\t\\n\\xa0\\\"\\'!?\\/\\(\\)%\\:\\=\\-\\+\\*\\_ๆ]', '', text)\n",
    "    \n",
    "    # Numbers. Replace by space\n",
    "    text = re.sub('[0-9]', ' ', text)\n",
    "    \n",
    "    # Dot. Replace by space\n",
    "    text = re.sub('[\\.]', ' ', text)\n",
    "    \n",
    "    # One or more consecutive space. Replace by single space\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    # Remove 2 or more repettition\n",
    "    text = remove_repettition(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "NNqVro6LPo8a"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "wv_model = gensim.models.Word2Vec.load('corpus.th.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6cfjBM4tP4UK"
   },
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    index = 0\n",
    "    index = wv_model.wv.vocab[word].index\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "oIAJw0p7P7XZ"
   },
   "outputs": [],
   "source": [
    "def word_index(listword):\n",
    "    dataset = []\n",
    "    vocabulary = dict()\n",
    "    inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "    for sentence in listword:\n",
    "        tmp = []\n",
    "        for w in sentence:\n",
    "            if w not in wv_model:\n",
    "                continue\n",
    "\n",
    "            if w not in vocabulary:\n",
    "                vocabulary[w] = len(inverse_vocabulary)\n",
    "                tmp.append(len(inverse_vocabulary))\n",
    "                inverse_vocabulary.append(w)\n",
    "            else:\n",
    "                tmp.append(word2idx(w))\n",
    "        dataset.append(tmp)\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "LO-WyseOP-li"
   },
   "outputs": [],
   "source": [
    "# define word embedding\n",
    "vocab_list = [(k, wv_model.wv[k]) for k, v in wv_model.wv.vocab.items()]\n",
    "embeddings_matrix = np.zeros((len(wv_model.wv.vocab.items()) + 1, wv_model.vector_size))\n",
    "for i in range(len(vocab_list)):\n",
    "    word = vocab_list[i][0]\n",
    "    embeddings_matrix[i + 1] = vocab_list[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1LVdY7-KDCP9EF2jZebOawVR_3jUQnSGM"
    },
    "executionInfo": {
     "elapsed": 6358,
     "status": "ok",
     "timestamp": 1620716838058,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "YzIv6bWzQC-a",
    "outputId": "14f29683-2dc8-45ec-f731-2f7f617dc726"
   },
   "outputs": [],
   "source": [
    "# vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "XdooUj6QQITq"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embeddings_matrix = 1 * np.random.randn(len(vocab_list) + 1, EMBEDDING_DIM)  # This will be the embedding matrix\n",
    "embeddings_matrix[0] = 0  # So that the padding will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0FdjJe_VPdZ0"
   },
   "outputs": [],
   "source": [
    "# Model variables\n",
    "n_hidden = 256\n",
    "batch_size = 128\n",
    "n_epoch = 100\n",
    "max_seq_length = 2704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1620716858559,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "eqK0ymH9QKRy",
    "outputId": "31ac3c38-d119-42ac-f01c-2d5c3a8a06fe"
   },
   "outputs": [],
   "source": [
    "# embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "an1uk86LQTio"
   },
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "JgPsQ8MlPfWA"
   },
   "outputs": [],
   "source": [
    "# The visible layer\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(len(embeddings_matrix), EMBEDDING_DIM, weights=[embeddings_matrix], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "# Embedded version of the inputs\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_lstm = LSTM(n_hidden)\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "# Pack it all up into a model\n",
    "malstm = Model([left_input, right_input], [malstm_distance])\n",
    "\n",
    "\n",
    "malstm.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Start training\n",
    "training_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1620717012710,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "Nh9milBGQpYY",
    "outputId": "6383054e-67f5-410f-8332-50281e0c54f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2704)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2704)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 2704, 300)    9468300     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          570368      embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "==================================================================================================\n",
      "Total params: 10,038,668\n",
      "Trainable params: 570,368\n",
      "Non-trainable params: 9,468,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "malstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "JUOlaUM6OylG"
   },
   "outputs": [],
   "source": [
    "# Load best weight from model\n",
    "malstm.load_weights('sm_colab_ka.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y69P6GtuXAaB"
   },
   "source": [
    "#Test with Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Kjaib296O0US"
   },
   "outputs": [],
   "source": [
    "def prepare_for_predict(input_questions):\n",
    "    q_input= []\n",
    "    cleansing(input_questions)\n",
    "    tokenized_input_1 =deepcut.tokenize(input_questions)\n",
    "    for sentence in tokenized_input_1:\n",
    "      q_input.append(sentence)\n",
    "    q_input= word_index(tokenized_input_1)\n",
    "    q_input = pad_sequences(q_input, maxlen=max_seq_length)\n",
    "    return q_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "CGdqwM2qWt0r"
   },
   "outputs": [],
   "source": [
    "max_word = 19219\n",
    "max_seq_length = 2704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate list\n",
    "def duplicate(testList, n):\n",
    "    return [ele for ele in testList for _ in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data from all category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_flag = False\n",
    "beforeTok={}\n",
    "tokenized = {}\n",
    "\n",
    "def getQuestions():\n",
    "    global beforeTok\n",
    "    global tokenized\n",
    "    while True:\n",
    "#         raw_questions = backendAPI()\n",
    "        raw_questions = getQuestionsFromBackendAPI() # <-- Uncomment this to get real questions\n",
    "        \n",
    "        # do tokenize\n",
    "        tokenize_questions = raw_questions\n",
    "        \n",
    "        curriculumDF = pd.DataFrame(data=tokenize_questions['หลักสูตร'])\n",
    "        curriculumDF = curriculumDF.rename(columns={0:\"curriculum\"})\n",
    "        \n",
    "        admissionDF = pd.DataFrame(data=tokenize_questions['การรับเข้านักศึกษา'])\n",
    "        admissionDF = admissionDF.rename(columns={0:\"admission\"})\n",
    "        \n",
    "        enrollmentDF = pd.DataFrame(data=tokenize_questions['ลงทะเบียนเรียน'])\n",
    "        enrollmentDF = enrollmentDF.rename(columns={0:\"enrollment\"})\n",
    "        \n",
    "        faqDF = pd.DataFrame(data=tokenize_questions['คำถามทั่วไป'])\n",
    "        faqDF = faqDF.rename(columns={0:\"faq\"})\n",
    "        \n",
    "        tokenized_enrollment =enrollmentDF.enrollment.map(tokenize_text_list)\n",
    "        tokenized_admission =admissionDF.admission.map(tokenize_text_list)\n",
    "        tokenized_curriculum =curriculumDF.curriculum.map(tokenize_text_list)\n",
    "        tokenized_faq =faqDF.faq.map(tokenize_text_list)\n",
    "        \n",
    "        tokenized = {}\n",
    "        tokenized['ลงทะเบียนเรียน'] = tokenized_enrollment\n",
    "        tokenized['การรับเข้านักศึกษา'] = tokenized_admission\n",
    "        tokenized['หลักสูตร'] = tokenized_curriculum\n",
    "        tokenized['คำถามทั่วไป'] = tokenized_faq\n",
    "        \n",
    "#         beforeTok = {}\n",
    "        beforeTok['ลงทะเบียนเรียน'] = enrollmentDF\n",
    "        beforeTok['การรับเข้านักศึกษา'] = admissionDF\n",
    "        beforeTok['หลักสูตร'] = curriculumDF\n",
    "        beforeTok['คำถามทั่วไป'] = faqDF\n",
    "        \n",
    "        # Update question\n",
    "        questions_data = tokenize_questions\n",
    "        if exit_flag: \n",
    "            break\n",
    "        \n",
    "        # Set query time\n",
    "        time.sleep(259200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://natthawat.live/api'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuestionsFromBackendAPI():\n",
    "    response = requests.get('%s/km/faq' % url)\n",
    "    faqs = json.loads(response.text)\n",
    "\n",
    "    response = requests.get('%s/km/category' % url)\n",
    "    categories = json.loads(response.text)\n",
    "\n",
    "    questions_data = {}\n",
    "    for category in categories:\n",
    "        questions_data[category['category']] = []\n",
    "        for faq in faqs:\n",
    "            if faq['category']['category'] == category['category']:\n",
    "                questions_data[category['category']].append(faq['question'])\n",
    "                \n",
    "    return questions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "getQuestionsThread = threading.Thread(target = getQuestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "getQuestionsThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ลงทะเบียนเรียน':                                            enrollment\n",
      "0   สามารถลงทะเบียนเกินหน่วยกิตที่หลักสูตรกำหนดได้...\n",
      "1    ใบแจ้งชำระเงินค่าลงทะเบียน สั่งพิมพ์ได้จากที่ไหน\n",
      "2                        ตอนดรอปต้องขอลายเซ็นใครบ้าง?\n",
      "3                       ขอทรานสคริปต์ออนไลน์ได้ที่ไหน\n",
      "4                     ยังต้องเข้าไปเรียนที่มหาลัยไหม?\n",
      "5       ลงทะเบียนเรียนเพิ่มรายวิชา ชำระเงินได้ที่ไหน?\n",
      "6   ชำระเงินค่าลงทะเบียนแล้ว แต่ในระบบยังขึ้นว่าไม...\n",
      "7   ถ้าจะดึงบางวิชาลงมาเรียนแทนตัวที่ติด f ได้ไหม ...\n",
      "8   ลืมจ่ายเงินค่าลงทะเบียน และ ใบแจ้งหนี้เกินกำหน...\n",
      "9   ถ้าต้องการลงทะเบียนเรียนน้อยกว่า 9 หน่วยกิตต้อ...\n",
      "10                   ถ้าการย้ายหลักสูตรต้องทำอย่างไร?\n",
      "11                            ตรวจวันเวลาสอบได้ที่ไหน\n",
      "12  ขอลดรายวิชาออนไลน์ หลังหมดเขต เพิ่ม-ลด 2 สัปดา...\n",
      "13  กรณีเพิ่มและหลักสูตรปิดรายวิชา จะได้รับเงินคืน...\n",
      "14                      แจ้งขอสำเร็จการศึกษาได้ที่ไหน\n",
      "15  ถ้าต้องการลงทะเบียนเรียนมากกว่า 19 หน่วยกิตต้อ...\n",
      "16                               วิชาเลือกไหนน่าสนใจ?\n",
      "17                              ลาป่วยต้องทำอย่างไร? \n",
      "18  หากมีเหตุต้องพักการเรียน สามารถพักการเรียนตอนน...\n",
      "19                   ต้องการสอบโดยไม่เข้าเรียนได้ไหม?\n",
      "20            ถ้าต้องการเปลี่ยน section ต้องทำอย่างไร\n",
      "21                หากจบการศึกษาไม่ทันเกณฑ์ทหารทำยังไง\n",
      "22               สามารถเช็คคอร์สเรียนออนไลน์ได้ที่ไหน\n",
      "23          ลงทะเบียนล่าช้าหลังระบบลงทะเบียนปิดที่ไหน\n",
      "24    ฉันสามารถลงวิชาเสริมในเทอมนี้ได้มากที่สุดกี่ตัว\n",
      "25                                ลากิจต้องทำอย่างไร?\n",
      "26                            ผมอยากลาออกต้องทำยังไง?\n",
      "27                   ถ้าวิชาเรียน sec เต็มแล้วทำยังไง\n",
      "28  กรณีเพิ่มและลดรายวิชา จะได้รับเงินคืนไหม หรือ ...\n",
      "29                               ดรอปได้ถึงเมื่อไหร่?\n",
      "30         สามารถเช็คกำหนดการลงทะเบียนเรียนได้ที่ไหน?\n",
      "31              ลงทะเบียนเรียนล่าช้าใช้เอกสารอะไรบ้าง\n",
      "32                           ตารางสอบซ้อนต้องทำยังไง?\n",
      "33         อยากได้เอกสารเบิกค่าเทอมต้องเตรียมอะไรบ้าง\n",
      "34            เรียนต่อต่างประเทศต้องใช้เอกสารอะไรบ้าง\n",
      "35  ถ้าจะต้องการเอาเครื่องคิดเลขเข้้าห้องสอบต้องทำ...\n",
      "36     ถ้าจะจบการศึกษา แต่เรียน gen ซ้ำหมวดกันทำยังไง, 'การรับเข้านักศึกษา':                                             admission\n",
      "0           วิศวะคอม/ TCAS รอบ 4 ใช้คะแนนสอบอะไรบ้าง?\n",
      "1            วิศวะคอม/ TCAS รอบ 3ใช้คะแนนสอบอะไรบ้าง?\n",
      "2                  หลักสูตรHDSมีเกณฑ์การรับยังไงบ้าง?\n",
      "3   การับสมัครเด็กเข้าเรียน มีทั้งหมดกี่รอบ? อะไรบ...\n",
      "4   สามารถติดต่อข่าวสารช่องทางการรับสมัครได้ที่ช่อ...\n",
      "5         แต่ละหลักสูตรมีเกณฑ์การรับพื้นฐานยังไงบ้าง?\n",
      "6             หลักสูตรนานาชาติมีเกณฑ์การรับยังไงบ้าง?\n",
      "7                 หลักสูตรปกติมีเกณฑ์การรับยังไงบ้าง?\n",
      "8   ถ้าจะสมัครเข้าโครงการ Active Recruitment ใช้เอ...\n",
      "9   ถ้าสนใจจะเรียนวิศวะคอม/ เปิดรับเมื่อไหร่ ช่วงไ...\n",
      "10      วิศวะคอม/ TCAS รอบ 1 มีโครงการไหนเปิดรับบ้าง?\n",
      "11  วิศวะคอม/ สามารถสมัครเข้าเรียนยังไงได้บ้าง มีร...\n",
      "12      วิศวะคอม/ TCAS รอบ 2 มีโครงการไหนเปิดรับบ้าง?\n",
      "13    สมัครเข้าโครงการ Active Recruitment ได้อย่างไร?\n",
      "14  ถ้าจะสมัครเข้าโครงการ Active Recruitment วัดผล...\n",
      "15         จะต้องใช้เอกสารอะไรบ้างในการสมัครเข้าศึกษา, 'หลักสูตร':                                            curriculum\n",
      "0   วิชาภาคบังคับที่มีวิชาต่อเนื่อง มีอะไรบ้าง หลั...\n",
      "1   ถ้าไม่ได้เป็นนักศึกษาชั้นปีที่ 3 สามารถฝึกงานไ...\n",
      "2           วิชา CPE223/Digital มีวิชาตัวต่อมั้ยครับ?\n",
      "3   วิชา CPE332/Professional issuesมีวิชาตัวต่อมั้...\n",
      "4   วิศวะคอม/หลักสูตรวิทยาศาสตรบัณฑิต สาขาวิชาวิทย...\n",
      "..                                                ...\n",
      "77  วิศวกรรมคอมพิวเตอร์หลักสูตรเรสิเดนทอล คอเลจ ปี...\n",
      "78  วิศวะคอม/หลักสูตรปกติ  ปี 3 เทอม 2\\nต้องลงเรีย...\n",
      "79  วิศวกรรมคอมพิวเตอร์หลักสูตรนานาชาติ ปีที่1-2 เ...\n",
      "80         วิชา CPE122/Circuits มีวิชาตัวต่อมั้ยครับ?\n",
      "81        วิศวะคอมหลักสูตรปกติมีอัตราค่าเรียนเท่าไหร่\n",
      "\n",
      "[82 rows x 1 columns], 'คำถามทั่วไป':                                                   faq\n",
      "0   การส่งคะแนนสอบภาษาอังกฤษเพื่อสำเร็จการศึกษา ส่...\n",
      "1             ติดต่อวิศวะคอม/ได้ทางช่องทางไหนได้บ้าง?\n",
      "2                      การสมัครกู้ยืมเรียน ทำอย่างไร?\n",
      "3                 เวลาต้องการยื่นเอกสารต้องยื่นที่ไหน\n",
      "4             ขั้นตอนและช่วงเวลาเปิดรับสมัครนักศึกษา?\n",
      "5                เกียรตินิยมอันดับ 2 อยู่ที่เท่าไหร่?\n",
      "6           วิศวะคอม/หลักสูตรปกติจบแล้วได้ปริญญาอะไร?\n",
      "7                              วิศวะคอมจบไปทำงานอะไร?\n",
      "8           หลักสูตรนานาชาติจบมาแล้วทำงานในสายใดบ้าง?\n",
      "9                     ใครเป็นหัวหน้าภาควิชาวิศวะคอม/?\n",
      "10                               วิศวะคอม/อยู่ที่ไหน?\n",
      "11                               มีที่ฝึกงานแนะนำไหม?\n",
      "12  ถ้าอยากไปเรียนต่อต่างและขอทุนต่างประเมศต้องทำอ...\n",
      "13                               สามารถขอทุนอะไรบ้าง?\n",
      "14      วิศวะคอม/หลักสูตรนานาชาติจบแล้วได้ปริญญาอะไร?\n",
      "15                      ปัจจุบันมีทุนอะไรเปิดรับบ้าง?\n",
      "16                   สามารถติดต่ออ.ได้ช่องทางไหนบ้าง?\n",
      "17  สามารถติดต่อได้ทางช่องทางไหนบ้าง ช่วงเวลากี่โม...\n",
      "18                 ทุนเพชรพระจอมเกล้าภาควิชารับกี่คน？\n",
      "19                                 ทุนจ้างงานคืออะไร？\n",
      "20  หลักสูตรวิทยาศาสตร์ข้อมูลสุขภาพจบมาแล้วทำงานใน...\n",
      "21               อาจารย์ในภาควิชาวิศวะคอม/มีใครบ้าง? \n",
      "22  วิศวะคอม/หลักสูตรวิทยาศาสตร์ข้อมูลสุขภาพจบแล้ว...\n",
      "23           หลักสูตรวิิทยาศาสตร์ข้อมูลสุขภาพคืออะไร?\n",
      "24                        วิศวะคอม/แล้วได้ปริญญาอะไร?\n",
      "25  เรียนจบต้องใช้คะแนนอังกฤษอะไรบ้าง ขั้นตํ่าเท่า...\n",
      "26               เกียรตินิยมอันดับ 1 อยู่ที่เท่าไหร่?\n",
      "27                     มีเงินไม่พอจ่าย ทำอะไรได้บ้าง?\n",
      "28                    วิศวะคอม/แตกต่างจาก IT อย่างไร?\n",
      "29                        วิศวะคอมเรียนเกี่ยวกับอะไร?\n",
      "30              หลักสูตรปกติจบมาแล้วทำงานในสายใดบ้าง?\n",
      "31              ขั้นตอนการรับสมัครโครงการทุนการศึกษา?\n",
      "32                           ทุนของภาควิชามีอะไรบ้าง？\n",
      "33  หลักสูตรเรสิเดรเทียล คอเจน์จบมาแล้วทำงานในสายใ...\n",
      "34  วิศวะคอม/หลักสูตรเรสิเดนทอล คอลเลจจบแล้วได้ปริ...\n",
      "35                                สมัครหลักสูตรยังไง?}\n"
     ]
    }
   ],
   "source": [
    "print(beforeTok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9rvOcqWW5sY"
   },
   "source": [
    "# time total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "executionInfo": {
     "elapsed": 1243,
     "status": "ok",
     "timestamp": 1620718089261,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "BjaHxfYqRSKQ",
    "outputId": "9a22dc57-b706-458c-c6ff-ea6fb0e664dd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NATTHAWATTUNGRUETHAI\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\NATTHAWATTUNGRUETHAI\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputQuestion = \"วิศวคอมมีหลักสูตรอะไรบ้าง\"\n",
    "# print('input: ' + inputQuestion)\n",
    "\n",
    "#category model\n",
    "inputQuestion= cleansing(inputQuestion)\n",
    "tokenized_input_2 = inputQuestion\n",
    "tokenized_text = deepcut.tokenize(tokenized_input_2)\n",
    "x = text_to_bow([tokenized_text], vocabulary_)\n",
    "x_tfidf = transformer.transform(x)\n",
    "x_svd = svd_model.transform(x_tfidf)\n",
    "pred = [model.predict_proba(x_svd.reshape(-1, 1).T).ravel()[1] for model in logist_models]\n",
    "\n",
    "# print(list(zip(tag, pred)))\n",
    "predict_category = max(list(zip(tag, pred)))\n",
    "max_value = 0\n",
    "max_category = ''\n",
    "pred_results = list(zip(tag, pred))\n",
    "\n",
    "for pred_result in pred_results:\n",
    "    # print(pred_result)\n",
    "    if pred_result[1] > max_value:\n",
    "        max_value = pred_result[1]\n",
    "        max_category = pred_result[0]\n",
    "# print(max_category, max_value)\n",
    "# end of category model\n",
    "\n",
    "\n",
    "# print(tokenized[max_category])\n",
    "\n",
    "\n",
    "# prediction\n",
    "tokenized_category = tokenized[max_category]\n",
    "# print(tokenized_category)\n",
    "max_word = 19219\n",
    "max_seq_length = 2704\n",
    "q_category= []\n",
    "for sentence in tokenized_category:\n",
    "    q_category.append(sentence)\n",
    "q_category = word_index(q_category)\n",
    "all_Question_categorylen = len(q_category)\n",
    "# all_Question_categorylen\n",
    "\n",
    "tokenized_dup_input_2= duplicate([tokenized_input_2],all_Question_categorylen)\n",
    "# print(tokenized_dup_input_2)\n",
    "\n",
    "q_user = word_index(tokenized_dup_input_2)\n",
    "# Split to dicts\n",
    "M_input = {'left': q_category, 'right': q_user}\n",
    "# Zero padding\n",
    "for model_input, side in itertools.product([M_input], ['left', 'right']):\n",
    "    model_input[side] = pad_sequences(model_input[side], maxlen=max_seq_length)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert M_input['left'].shape == M_input['right'].shape\n",
    "play_predict = malstm.predict(x=[M_input['left'], M_input['right']])\n",
    "\n",
    "max_question_percentage = max(play_predict)\n",
    "# print(max_question_percentage)\n",
    "question_index = np.where(play_predict == max_question_percentage)\n",
    "# print(question_index)\n",
    "predictedQuestion = beforeTok[max_category].loc[question_index[0][0]]\n",
    "predictedQuestion = predictedQuestion[0]\n",
    "\n",
    "value = {\n",
    "    \"category\": max_category,\n",
    "    \"accuracy\": \"%lf\" % max_value,\n",
    "    \"predictedQuestion\": str(predictedQuestion),\n",
    "    \"similarity\": \"%lf\" % max_question_percentage\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# time seperate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: วิศวคอมมีหลักสูตรอะไรบ้าง\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputQuestion = \"วิศวคอมมีหลักสูตรอะไรบ้าง\"\n",
    "print('input: ' + inputQuestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#category model\n",
    "tokenized_input_2= cleansing(inputQuestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_text = deepcut.tokenize(tokenized_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = text_to_bow([tokenized_text], vocabulary_)\n",
    "x_tfidf = transformer.transform(x)\n",
    "x_svd = svd_model.transform(x_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred = [model.predict_proba(x_svd.reshape(-1, 1).T).ravel()[1] for model in logist_models]\n",
    "# print(list(zip(tag, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict_category = max(list(zip(tag, pred)))\n",
    "max_value = 0\n",
    "max_category = ''\n",
    "pred_results = list(zip(tag, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "หลักสูตร 0.430342087763526\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for pred_result in pred_results:\n",
    "    # print(pred_result)\n",
    "    if pred_result[1] > max_value:\n",
    "        max_value = pred_result[1]\n",
    "        max_category = pred_result[0]\n",
    "print(max_category, max_value)\n",
    "# end of category model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# prediction\n",
    "tokenized_category = tokenized[max_category]\n",
    "# print(tokenized_category)\n",
    "max_word = 19219\n",
    "max_seq_length = 2704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.01 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NATTHAWATTUNGRUETHAI\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\NATTHAWATTUNGRUETHAI\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q_category= []\n",
    "for sentence in tokenized_category:\n",
    "    q_category.append(sentence)\n",
    "q_category = word_index(q_category)\n",
    "all_Question_categorylen = len(q_category)\n",
    "# all_Question_categorylen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_input_2 = deepcut.tokenize(inputQuestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_dup_input_2= duplicate([tokenized_input_2],all_Question_categorylen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.99 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NATTHAWATTUNGRUETHAI\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q_user = word_index(tokenized_dup_input_2)\n",
    "# Split to dicts\n",
    "M_input = {'left': q_category, 'right': q_user}\n",
    "# Zero padding\n",
    "for model_input, side in itertools.product([M_input], ['left', 'right']):\n",
    "    model_input[side] = pad_sequences(model_input[side], maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make sure everything is ok\n",
    "assert M_input['left'].shape == M_input['right'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 707 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "play_predict = malstm.predict(x=[M_input['left'],  M_input['right']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.973298]\n",
      "(array([42], dtype=int64), array([0], dtype=int64))\n",
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_question_percentage = max(play_predict)\n",
    "print(max_question_percentage)\n",
    "question_index = np.where(play_predict == max_question_percentage)\n",
    "print(question_index)\n",
    "predictedQuestion = beforeTok[max_category].loc[question_index[0][0]]\n",
    "predictedQuestion = predictedQuestion[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "value = {\n",
    "    \"category\": max_category,\n",
    "    \"accuracy\": \"%lf\" % max_value,\n",
    "    \"predictedQuestion\": str(predictedQuestion),\n",
    "    \"similarity\": \"%lf\" % max_question_percentage\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Siamese_malstm_play.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
