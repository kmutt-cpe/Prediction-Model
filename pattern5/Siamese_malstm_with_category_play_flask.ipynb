{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.19.5 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.19.5 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py==2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from h5py==2.10.0) (1.19.5)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from h5py==2.10.0) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py==2.10.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.6.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (3.6.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from gensim==3.6.0) (5.0.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim==3.6.0) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from gensim==3.6.0) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim==3.6.0) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==3.6.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4035,
     "status": "ok",
     "timestamp": 1620716501332,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "wxRqFag-N8dW",
    "outputId": "b5211c30-c91c-472d-f826-057ad5c61271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepcut in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (0.7.0.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from deepcut) (1.5.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from deepcut) (1.19.5)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from deepcut) (1.1.3)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from deepcut) (2.10.0)\n",
      "Requirement already satisfied: tensorflow>=2.0.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from deepcut) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from deepcut) (0.23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->deepcut) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->deepcut) (2020.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from h5py->deepcut) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (3.3.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.1.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.32.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (2.5.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (2.4.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (0.3.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (0.35.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (0.12.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=2.0.0->deepcut) (1.12)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (3.16.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.0.0->deepcut) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->deepcut) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->deepcut) (0.17.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (2.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (50.3.1.post20201107)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (1.30.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (4.7.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->deepcut) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install deepcut --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5877,
     "status": "ok",
     "timestamp": 1620716503182,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "rHO_3T9BOGui",
    "outputId": "1a44454f-1da2-4648-cdbb-c18110a79609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pythainlp in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (2.3.1)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from pythainlp) (0.9.7)\n",
      "Requirement already satisfied: tinydb>=3.0 in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python38\\site-packages (from pythainlp) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pythainlp) (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pythainlp) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pythainlp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python37\\site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in c:\\users\\natthawattungruethai\\appdata\\roaming\\python\\python37\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\natthawattungruethai\\.conda\\envs\\tensorflow\\lib\\site-packages (from nltk) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlrd\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Installing collected packages: xlrd\n",
      "Successfully installed xlrd-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.7-py2.py3-none-any.whl (243 kB)\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G9D118ZAOZZB"
   },
   "outputs": [],
   "source": [
    "#prediction\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "import deepcut\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question import\n",
    "import requests\n",
    "\n",
    "# Category model import\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask_ngrok import run_with_ngrok\n",
    "from flask import Flask, jsonify, request\n",
    "\n",
    "import threading \n",
    "import json\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Your version of xlrd is 2.0.1. In xlrd >= 2.0, only the xls format is supported. Install openpyxl instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-41d0a0eb0e12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Category.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m                 )\n\u001b[0;32m    298\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"xls\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mxlrd_version\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;34m\"2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 raise ValueError(\n\u001b[1;32m-> 1103\u001b[1;33m                     \u001b[1;34mf\"Your version of xlrd is {xlrd_version}. In xlrd >= 2.0, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m                     \u001b[1;34mf\"only the xls format is supported. Install openpyxl instead.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: Your version of xlrd is 2.0.1. In xlrd >= 2.0, only the xls format is supported. Install openpyxl instead."
     ]
    }
   ],
   "source": [
    "data = pd.read_excel(\"Category.xlsx\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load File\n",
    "with open('token_text_category.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    tokenized_texts = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pL8GEUFCWEKq"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def tokenize_text_list(ls):\n",
    "    \"\"\"Tokenize list of text\"\"\"\n",
    "    return list(chain.from_iterable([deepcut.tokenize(ls)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bow(tokenized_text, vocabulary_):\n",
    "    n_doc = len(tokenized_text)\n",
    "    values, row_indices, col_indices = [], [], []\n",
    "    for r, tokens in enumerate(tokenized_text):\n",
    "        feature = {}\n",
    "        for token in tokens:\n",
    "            word_index = vocabulary_.get(token)\n",
    "            if word_index is not None:\n",
    "                if word_index not in feature.keys():\n",
    "                    feature[word_index] = 1\n",
    "                else:\n",
    "                    feature[word_index] += 1\n",
    "        for c, v in feature.items():\n",
    "            values.append(v)\n",
    "            row_indices.append(r)\n",
    "            col_indices.append(c)\n",
    "        #print(feature)\n",
    "\n",
    "    # document-term matrix in sparse CSR format\n",
    "    X = sp.csr_matrix((values, (row_indices, col_indices)),\n",
    "                      shape=(n_doc, len(vocabulary_)))\n",
    "    return X\n",
    "\n",
    "vocabulary_ = {v: k for k, v in enumerate(set(chain.from_iterable(tokenized_texts)))}\n",
    "X = text_to_bow(tokenized_texts, vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "svd_model = TruncatedSVD(n_components=100,\n",
    "                         algorithm='arpack', n_iter=100)\n",
    "X_tfidf = transformer.fit_transform(X)\n",
    "X_svd = svd_model.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = pd.get_dummies(data.Category).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lib\n",
    "import joblib\n",
    "\n",
    "#Load Model\n",
    "logist_models = joblib.load(\"category_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['การรับเข้านักศึกษา', 'คำถามทั่วไป', 'ลงทะเบียนเรียน', 'หลักสูตร'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(np.vstack([model.predict_proba(X_svd)[:, 1] for model in logist_models]).T, axis=1)\n",
    "y_pred = np.array([tag[yi] for yi in y_pred])\n",
    "y_true = data.Category.values\n",
    "print(tag[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5qw6hsPwOmcD"
   },
   "outputs": [],
   "source": [
    "#Clean Text\n",
    "def remove_repettition(text):\n",
    "    token_list = list(text)\n",
    "    if len(token_list) > 2:\n",
    "        filter_list = [True, True]\n",
    "        n = len(token_list)\n",
    "        for i in range(2, n):\n",
    "            if (token_list[i] == token_list[i-1]) and (token_list[i] == token_list[i-2]):\n",
    "                filter_list.append(False)\n",
    "            else:\n",
    "                filter_list.append(True)\n",
    "\n",
    "        output = ''.join(np.array(token_list)[filter_list])\n",
    "    else:\n",
    "        output = text\n",
    "    return output\n",
    "\n",
    "def cleansing(text):\n",
    "    # \\t, \\n, \\xa0 and other special characters. Replace by blank string\n",
    "    text = re.sub('[\\t\\n\\xa0\\\"\\'!?\\/\\(\\)%\\:\\=\\-\\+\\*\\_ๆ]', '', text)\n",
    "    \n",
    "    # Numbers. Replace by space\n",
    "    text = re.sub('[0-9]', ' ', text)\n",
    "    \n",
    "    # Dot. Replace by space\n",
    "    text = re.sub('[\\.]', ' ', text)\n",
    "    \n",
    "    # One or more consecutive space. Replace by single space\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    # Remove 2 or more repettition\n",
    "    text = remove_repettition(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NNqVro6LPo8a"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "wv_model = gensim.models.Word2Vec.load('corpus.th.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6cfjBM4tP4UK"
   },
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    index = 0\n",
    "    index = wv_model.wv.vocab[word].index\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "oIAJw0p7P7XZ"
   },
   "outputs": [],
   "source": [
    "def word_index(listword):\n",
    "    dataset = []\n",
    "    vocabulary = dict()\n",
    "    inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "    for sentence in listword:\n",
    "        tmp = []\n",
    "        for w in sentence:\n",
    "            if w not in wv_model:\n",
    "                continue\n",
    "\n",
    "            if w not in vocabulary:\n",
    "                vocabulary[w] = len(inverse_vocabulary)\n",
    "                tmp.append(len(inverse_vocabulary))\n",
    "                inverse_vocabulary.append(w)\n",
    "            else:\n",
    "                tmp.append(word2idx(w))\n",
    "        dataset.append(tmp)\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LO-WyseOP-li"
   },
   "outputs": [],
   "source": [
    "# define word embedding\n",
    "vocab_list = [(k, wv_model.wv[k]) for k, v in wv_model.wv.vocab.items()]\n",
    "embeddings_matrix = np.zeros((len(wv_model.wv.vocab.items()) + 1, wv_model.vector_size))\n",
    "for i in range(len(vocab_list)):\n",
    "    word = vocab_list[i][0]\n",
    "    embeddings_matrix[i + 1] = vocab_list[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1LVdY7-KDCP9EF2jZebOawVR_3jUQnSGM"
    },
    "executionInfo": {
     "elapsed": 6358,
     "status": "ok",
     "timestamp": 1620716838058,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "YzIv6bWzQC-a",
    "outputId": "14f29683-2dc8-45ec-f731-2f7f617dc726"
   },
   "outputs": [],
   "source": [
    "# vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "XdooUj6QQITq"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embeddings_matrix = 1 * np.random.randn(len(vocab_list) + 1, EMBEDDING_DIM)  # This will be the embedding matrix\n",
    "embeddings_matrix[0] = 0  # So that the padding will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0FdjJe_VPdZ0"
   },
   "outputs": [],
   "source": [
    "# Model variables\n",
    "n_hidden = 256\n",
    "batch_size = 128\n",
    "n_epoch = 100\n",
    "max_seq_length = 2704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1620716858559,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "eqK0ymH9QKRy",
    "outputId": "31ac3c38-d119-42ac-f01c-2d5c3a8a06fe"
   },
   "outputs": [],
   "source": [
    "# embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "an1uk86LQTio"
   },
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "JgPsQ8MlPfWA"
   },
   "outputs": [],
   "source": [
    "# The visible layer\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(len(embeddings_matrix), EMBEDDING_DIM, weights=[embeddings_matrix], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "# Embedded version of the inputs\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_lstm = LSTM(n_hidden)\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "# Pack it all up into a model\n",
    "malstm = Model([left_input, right_input], [malstm_distance])\n",
    "\n",
    "\n",
    "malstm.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Start training\n",
    "training_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1620717012710,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "Nh9milBGQpYY",
    "outputId": "6383054e-67f5-410f-8332-50281e0c54f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2704)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2704)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 2704, 300)    9468300     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          570368      embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "==================================================================================================\n",
      "Total params: 10,038,668\n",
      "Trainable params: 570,368\n",
      "Non-trainable params: 9,468,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "malstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "JUOlaUM6OylG"
   },
   "outputs": [],
   "source": [
    "# Load best weight from model\n",
    "malstm.load_weights('sm_colab_ka.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y69P6GtuXAaB"
   },
   "source": [
    "#Test with Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Kjaib296O0US"
   },
   "outputs": [],
   "source": [
    "def prepare_for_predict(input_questions):\n",
    "    q_input= []\n",
    "    cleansing(input_questions)\n",
    "    tokenized_input_1 =deepcut.tokenize(input_questions)\n",
    "    for sentence in tokenized_input_1:\n",
    "      q_input.append(sentence)\n",
    "    q_input= word_index(tokenized_input_1)\n",
    "    q_input = pad_sequences(q_input, maxlen=max_seq_length)\n",
    "    return q_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "CGdqwM2qWt0r"
   },
   "outputs": [],
   "source": [
    "max_word = 19219\n",
    "max_seq_length = 2704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate list\n",
    "def duplicate(testList, n):\n",
    "    return [ele for ele in testList for _ in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data from all category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_flag = False\n",
    "beforeTok={}\n",
    "tokenized = {}\n",
    "\n",
    "def getQuestions():\n",
    "    global beforeTok\n",
    "    global tokenized\n",
    "    while True:\n",
    "#         raw_questions = backendAPI()\n",
    "        raw_questions = getQuestionsFromBackendAPI() # <-- Uncomment this to get real questions\n",
    "        \n",
    "        # do tokenize\n",
    "        tokenize_questions = raw_questions\n",
    "        \n",
    "        curriculumDF = pd.DataFrame(data=tokenize_questions['หลักสูตร'])\n",
    "        curriculumDF = curriculumDF.rename(columns={0:\"curriculum\"})\n",
    "        \n",
    "        admissionDF = pd.DataFrame(data=tokenize_questions['การรับเข้านักศึกษา'])\n",
    "        admissionDF = admissionDF.rename(columns={0:\"admission\"})\n",
    "        \n",
    "        enrollmentDF = pd.DataFrame(data=tokenize_questions['ลงทะเบียนเรียน'])\n",
    "        enrollmentDF = enrollmentDF.rename(columns={0:\"enrollment\"})\n",
    "        \n",
    "        faqDF = pd.DataFrame(data=tokenize_questions['คำถามทั่วไป'])\n",
    "        faqDF = faqDF.rename(columns={0:\"faq\"})\n",
    "        \n",
    "        tokenized_enrollment =enrollmentDF.enrollment.map(tokenize_text_list)\n",
    "        tokenized_admission =admissionDF.admission.map(tokenize_text_list)\n",
    "        tokenized_curriculum =curriculumDF.curriculum.map(tokenize_text_list)\n",
    "        tokenized_faq =faqDF.faq.map(tokenize_text_list)\n",
    "        \n",
    "        tokenized = {}\n",
    "        tokenized['ลงทะเบียนเรียน'] = tokenized_enrollment\n",
    "        tokenized['การรับเข้านักศึกษา'] = tokenized_admission\n",
    "        tokenized['หลักสูตร'] = tokenized_curriculum\n",
    "        tokenized['คำถามทั่วไป'] = tokenized_faq\n",
    "        \n",
    "#         beforeTok = {}\n",
    "        beforeTok['ลงทะเบียนเรียน'] = enrollmentDF\n",
    "        beforeTok['การรับเข้านักศึกษา'] = admissionDF\n",
    "        beforeTok['หลักสูตร'] = curriculumDF\n",
    "        beforeTok['คำถามทั่วไป'] = faqDF\n",
    "        \n",
    "        # Update question\n",
    "        questions_data = tokenize_questions\n",
    "        if exit_flag: \n",
    "            break\n",
    "        \n",
    "        # Set query time\n",
    "        time.sleep(259200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://natthawat.live/api'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuestionsFromBackendAPI():\n",
    "    response = requests.get('%s/km/faq' % url)\n",
    "    faqs = json.loads(response.text)\n",
    "\n",
    "    response = requests.get('%s/km/category' % url)\n",
    "    categories = json.loads(response.text)\n",
    "\n",
    "    questions_data = {}\n",
    "    for category in categories:\n",
    "        questions_data[category['category']] = []\n",
    "        for faq in faqs:\n",
    "            if faq['category']['category'] == category['category']:\n",
    "                questions_data[category['category']].append(faq['question'])\n",
    "                \n",
    "    return questions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "getQuestionsThread = threading.Thread(target = getQuestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "getQuestionsThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(beforeTok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9rvOcqWW5sY"
   },
   "source": [
    "# Run API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "executionInfo": {
     "elapsed": 1243,
     "status": "ok",
     "timestamp": 1620718089261,
     "user": {
      "displayName": "Natkanok Poksappaiboon",
      "photoUrl": "",
      "userId": "02284855325556491333"
     },
     "user_tz": -420
    },
    "id": "BjaHxfYqRSKQ",
    "outputId": "9a22dc57-b706-458c-c6ff-ea6fb0e664dd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [19/May/2021 03:34:45] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [19/May/2021 03:34:50] \"\u001b[37mGET /prediction HTTP/1.1\u001b[0m\" 200 -\n",
      "<ipython-input-21-e36182fd4f37>:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if w not in wv_model:\n",
      "<ipython-input-21-e36182fd4f37>:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(dataset)\n",
      "127.0.0.1 - - [19/May/2021 03:35:16] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "@app.route('/prediction', methods=['POST', 'GET'])\n",
    "def home():\n",
    "    if request.method == 'POST':\n",
    "        data = request.get_json()\n",
    "        inputQuestion = data[\"inputQuestion\"]\n",
    "        #category model\n",
    "        inputQuestion= cleansing(inputQuestion)\n",
    "        tokenized_input_2 = inputQuestion\n",
    "        tokenized_text = deepcut.tokenize(tokenized_input_2)\n",
    "        x = text_to_bow([tokenized_text], vocabulary_)\n",
    "        x_tfidf = transformer.transform(x)\n",
    "        x_svd = svd_model.transform(x_tfidf)\n",
    "        pred = [model.predict_proba(x_svd.reshape(-1, 1).T).ravel()[1] for model in logist_models]\n",
    "\n",
    "        # print(list(zip(tag, pred)))\n",
    "        predict_category = max(list(zip(tag, pred)))\n",
    "        max_value = 0\n",
    "        max_category = ''\n",
    "        pred_results = list(zip(tag, pred))\n",
    "\n",
    "        for pred_result in pred_results:\n",
    "            # print(pred_result)\n",
    "            if pred_result[1] > max_value:\n",
    "                max_value = pred_result[1]\n",
    "                max_category = pred_result[0]\n",
    "        # print(max_category, max_value)\n",
    "        # end of category model\n",
    "\n",
    "\n",
    "        # print(tokenized[max_category])\n",
    "\n",
    "\n",
    "        # prediction\n",
    "        tokenized_category = tokenized[max_category]\n",
    "        # print(tokenized_category)\n",
    "        max_word = 19219\n",
    "        max_seq_length = 2704\n",
    "        q_category= []\n",
    "        for sentence in tokenized_category:\n",
    "            q_category.append(sentence)\n",
    "        q_category = word_index(q_category)\n",
    "        all_Question_categorylen = len(q_category)\n",
    "        # all_Question_categorylen\n",
    "\n",
    "        tokenized_dup_input_2= duplicate([tokenized_input_2],all_Question_categorylen)\n",
    "        # print(tokenized_dup_input_2)\n",
    "\n",
    "        q_user = word_index(tokenized_dup_input_2)\n",
    "        # Split to dicts\n",
    "        M_input = {'left': q_category, 'right': q_user}\n",
    "        # Zero padding\n",
    "        for model_input, side in itertools.product([M_input], ['left', 'right']):\n",
    "            model_input[side] = pad_sequences(model_input[side], maxlen=max_seq_length)\n",
    "\n",
    "        # Make sure everything is ok\n",
    "        assert M_input['left'].shape == M_input['right'].shape\n",
    "        play_predict = malstm.predict(x=[M_input['left'], M_input['right']])\n",
    "\n",
    "        max_question_percentage = max(play_predict)\n",
    "        # print(max_question_percentage)\n",
    "        question_index = np.where(play_predict == max_question_percentage)\n",
    "        # print(question_index)\n",
    "        predictedQuestion = beforeTok[max_category].loc[question_index[0][0]]\n",
    "        predictedQuestion = predictedQuestion[0]\n",
    "\n",
    "        value = {\n",
    "            \"category\": max_category,\n",
    "            \"accuracy\": \"%lf\" % max_value,\n",
    "            \"predictedQuestion\": str(predictedQuestion),\n",
    "            \"similarity\": \"%lf\" % max_question_percentage\n",
    "        }\n",
    "        return json.dumps(value, ensure_ascii=False).encode('utf8')\n",
    "    else:\n",
    "        return \"Hello. I am alive!\"\n",
    "\n",
    "app.run(port=3000,debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example of api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputQuestion': 'วิศวคอมมีหลักสูตรอะไรบ้าง'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "\"inputQuestion\":\"วิศวคอมมีหลักสูตรอะไรบ้าง\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Siamese_malstm_play.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
